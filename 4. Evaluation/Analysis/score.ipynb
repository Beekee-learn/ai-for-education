{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from tqdm import tqdm\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import FactualCorrectness, SemanticSimilarity, StringPresence\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas.llms import LangchainLLMWrapper \n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Initialize Weave\n",
    "weave.init('benchmark')\n",
    "\n",
    "# Initialize LLM and Embeddings\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.environ['OPENAI_API_KEY']))\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(api_key=os.environ['OPENAI_API_KEY']))\n",
    "\n",
    "def load_test_data(benchmark_name):\n",
    "    \"\"\"Load the test data from Weave.\"\"\"\n",
    "    return weave.ref(benchmark_name).get()\n",
    "\n",
    "async def evaluate_sample(chat_response, expected, response):\n",
    "    \"\"\"Evaluate a single sample.\"\"\"\n",
    "    answer_sample = SingleTurnSample(response=chat_response, reference=expected)\n",
    "    exact_sample = SingleTurnSample(response=chat_response, reference=response)\n",
    "\n",
    "    if not chat_response:\n",
    "        return 0, 0, 0, 0\n",
    "\n",
    "    try:\n",
    "        factual = await FactualCorrectness().single_turn_ascore(answer_sample)\n",
    "        similar = await SemanticSimilarity().single_turn_ascore(answer_sample)\n",
    "        present = await StringPresence().single_turn_ascore(exact_sample)\n",
    "\n",
    "        factual = float(factual)\n",
    "        similar = float(similar)\n",
    "        present = float(present)\n",
    "\n",
    "        general_score = (0.5 * factual + 0.2 * similar + 0.3 * present)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating sample: {e}\")\n",
    "        return 0, 0, 0, 0\n",
    "\n",
    "    return general_score, factual, similar, present\n",
    "\n",
    "async def evaluate():\n",
    "    total_results = []\n",
    "    benchmark_name = 'gsm8k'\n",
    "    df = load_test_data(benchmark_name)\n",
    "    dataset_name = f'{benchmark_name}-llama_cpp-Llama_3-custom-raspberry_5'\n",
    "\n",
    "    for j in range(3, 123, 1000):\n",
    "        bot_answer = load_test_data(f'{dataset_name}-batch{j-1}-10')\n",
    "        start_index = j * 10 - 10\n",
    "        end_index = start_index + 10\n",
    "        results = []\n",
    "\n",
    "        for i in tqdm(range(start_index, min(end_index, len(df.rows)))):\n",
    "            prompt = df.rows[i]['prompt']\n",
    "            expected, response = df.rows[i]['expecting'].split('####')\n",
    "\n",
    "            chat_response = bot_answer.rows[i % 10]['response'][0]\n",
    "            general_score, factual, similar, present = await evaluate_sample(chat_response, expected, response)\n",
    "\n",
    "            results.append({\n",
    "                \"prompt\": {\n",
    "                    \"general\": general_score,\n",
    "                    \"factual_correctness\": factual,\n",
    "                    \"semantic_similarity\": similar,\n",
    "                    \"string_presence\": present\n",
    "                }\n",
    "            })\n",
    "\n",
    "        result = {\n",
    "            \"all_results\": results,\n",
    "            \"average\": {\n",
    "                \"general_results\": sum(x[\"prompt\"][\"general\"] for x in results) / len(results),\n",
    "                \"factual_correctness\": sum(x[\"prompt\"][\"factual_correctness\"] for x in results) / len(results),\n",
    "                \"semantic_similarity\": sum(x[\"prompt\"][\"semantic_similarity\"] for x in results) / len(results),\n",
    "                \"string_presence\": sum(x[\"prompt\"][\"string_presence\"] for x in results) / len(results)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        total_results.append(result)\n",
    "        with open(f'results_{j}.json', 'w') as f:\n",
    "            json.dump(result, f, indent=4)\n",
    "\n",
    "    with open('results.json', 'w') as f:\n",
    "        json.dump(total_results, f, indent=4)\n",
    "\n",
    "import asyncio\n",
    "asyncio.run(evaluate())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from tqdm import tqdm\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import LLMContextPrecisionWithoutReference, LLMContextRecall\n",
    "from langchain_openai import ChatOpenAI\n",
    "from ragas.llms import LangchainLLMWrapper \n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Initialize Weave\n",
    "weave.init('benchmark')\n",
    "\n",
    "# Initialize LLM and Embeddings\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.environ['OPEN_AI_API_KEY']))\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(api_key=os.environ['OPEN_AI_API_KEY']))\n",
    "\n",
    "def load_test_data(benchmark_name):\n",
    "    \"\"\"Load the test data from Weave.\"\"\"\n",
    "    return weave.ref(benchmark_name).get()\n",
    "\n",
    "async def evaluate_sample(chat_response, prompt, expected, context):\n",
    "    \"\"\"Evaluate a single sample for context precision and recall.\"\"\"\n",
    "    context_precision_sample = SingleTurnSample(\n",
    "        response=chat_response,\n",
    "        retrieved_contexts=context,\n",
    "        user_input=prompt\n",
    "    )\n",
    "    \n",
    "    context_recall_sample = SingleTurnSample(\n",
    "        response=chat_response,\n",
    "        reference=expected,\n",
    "        retrieved_contexts=context,\n",
    "        user_input=prompt\n",
    "    )\n",
    "    \n",
    "    scorer = LLMContextPrecisionWithoutReference()\n",
    "    scorer.llm = evaluator_llm\n",
    "    precision = float(await scorer.single_turn_ascore(context_precision_sample))\n",
    "    \n",
    "    scorer = LLMContextRecall()\n",
    "    scorer.llm = evaluator_llm\n",
    "    recall = float(await scorer.single_turn_ascore(context_recall_sample))\n",
    "    \n",
    "    general_score = (0.5 * precision + 0.5 * recall)\n",
    "    \n",
    "    return general_score, precision, recall\n",
    "\n",
    "async def evaluate():\n",
    "    benchmark_name = 'gsm8k'\n",
    "    df = load_test_data(benchmark_name)\n",
    "    dataset_name = \"gsm8k-llama_cpp-llama_3_8b_Q4_K_M-rag_2_sierra-raspberry_5-batch\"\n",
    "    \n",
    "    for j in range(1, 4):\n",
    "        bot_answer = load_test_data(f'{dataset_name}{j}-10')\n",
    "        start_index = 0\n",
    "        end_index = 10\n",
    "        results = []\n",
    "        \n",
    "        for i in tqdm(range(start_index, min(end_index, len(df.rows)))):\n",
    "            prompt = df.rows[i]['prompt']\n",
    "            context = list(bot_answer.rows[i]['context_q'])\n",
    "            expected, response = df.rows[i]['expecting'].split('####')    \n",
    "            chat_response = bot_answer.rows[i]['response']\n",
    "            \n",
    "            general_score, precision, recall = await evaluate_sample(chat_response, prompt, expected, context)\n",
    "            \n",
    "            results.append({\n",
    "                \"prompt\": {\n",
    "                    \"general\": general_score,\n",
    "                    \"context_precision\": precision,\n",
    "                    \"context_recall\": recall,\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        result = {\n",
    "            \"all_results\": results,\n",
    "            \"average\": {\n",
    "                \"general_retrieval\": sum([x[\"prompt\"][\"general\"] for x in results]) / len(results),\n",
    "                \"context_precision\": sum([x[\"prompt\"][\"context_precision\"] for x in results]) / len(results),\n",
    "                \"context_recall\": sum([x[\"prompt\"][\"context_recall\"] for x in results]) / len(results),\n",
    "            }\n",
    "        }\n",
    "\n",
    "        with open(f'retrieval_{j}.json', 'w') as f:\n",
    "            json.dump(result, f, indent=4)\n",
    "\n",
    "import asyncio\n",
    "asyncio.run(evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from tqdm import tqdm\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import Faithfulness\n",
    "from langchain_openai import ChatOpenAI\n",
    "from ragas.llms import LangchainLLMWrapper \n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Initialize Weave\n",
    "weave.init('benchmark')\n",
    "\n",
    "# Initialize LLM and Embeddings\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.environ['OPEN_AI_API_KEY']))\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(api_key=os.environ['OPEN_AI_API_KEY']))\n",
    "\n",
    "def load_test_data(benchmark_name):\n",
    "    \"\"\"Load the test data from Weave.\"\"\"\n",
    "    return weave.ref(benchmark_name).get()\n",
    "\n",
    "async def evaluate_sample(chat_response, prompt, context):\n",
    "    \"\"\"Evaluate a single sample for faithfulness.\"\"\"\n",
    "    faithful_sample = SingleTurnSample(\n",
    "        response=chat_response,\n",
    "        user_input=prompt,\n",
    "        retrieved_contexts=context\n",
    "    )\n",
    "    \n",
    "    if chat_response:\n",
    "        scorer = Faithfulness()\n",
    "        scorer.llm = evaluator_llm\n",
    "        score = float(await scorer.single_turn_ascore(faithful_sample))\n",
    "    else:\n",
    "        score = 0.0\n",
    "    \n",
    "    return score\n",
    "\n",
    "async def evaluate():\n",
    "    benchmark_name = 'gsm8k'\n",
    "    df = load_test_data(benchmark_name)\n",
    "    dataset_name = \"gsm8k-llama_cpp-llama_3_8b_Q4_K_M-rag_2_sierra-raspberry_5-batch\"\n",
    "    \n",
    "    for j in range(1, 4):\n",
    "        bot_answer = load_test_data(f'{dataset_name}{j}-10')\n",
    "        start_index = 0\n",
    "        end_index = 10\n",
    "        results = []\n",
    "        \n",
    "        for i in tqdm(range(start_index, min(end_index, len(df.rows)))):\n",
    "            prompt = df.rows[i]['prompt']\n",
    "            context = list(bot_answer.rows[i]['context_q'])\n",
    "            expected, response = df.rows[i]['expecting'].split('####')\n",
    "            chat_response = bot_answer.rows[i]['response']\n",
    "            \n",
    "            score = await evaluate_sample(chat_response, prompt, context)\n",
    "            \n",
    "            results.append({\n",
    "                \"prompt\": {\n",
    "                    \"general\": score,\n",
    "                    \"faithfulness\": score\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        result = {\n",
    "            \"all_results\": results,\n",
    "            \"average\": {\n",
    "                \"general_usage\": sum(x[\"prompt\"][\"general\"] for x in results) / len(results),\n",
    "                \"faithfulness\": sum(x[\"prompt\"][\"faithfulness\"] for x in results) / len(results),\n",
    "            }\n",
    "        }\n",
    "\n",
    "        with open(f'usage_{j}.json', 'w') as f:\n",
    "            json.dump(result, f, indent=4)\n",
    "\n",
    "import asyncio\n",
    "asyncio.run(evaluate())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
